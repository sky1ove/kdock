{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# basics\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm import tqdm\n",
    "import hashlib,numba\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "from skfp.fingerprints import AtomPairFingerprint, ECFPFingerprint, MACCSFingerprint\n",
    "from kdock.data.core import Data\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # suppress umap tensorflow warning\n",
    "from umap import UMAP  # import UMAP class properly\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors,Descriptors3D, AllChem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rdkit feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_rdkit(SMILES:str):\n",
    "    \"\"\"\n",
    "    Extract chemical features from SMILES\n",
    "    Reference: https://greglandrum.github.io/rdkit-blog/posts/2022-12-23-descriptor-tutorial.html\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(SMILES)\n",
    "    return Descriptors.CalcMolDescriptors(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_rdkit_3d(SMILES:str):\n",
    "    \"Extract 3d features from SMILES\"\n",
    "    mol = Chem.MolFromSmiles(SMILES)\n",
    "    mol = Chem.AddHs(mol)\n",
    "    AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
    "    AllChem.UFFOptimizeMolecule(mol)\n",
    "    return Descriptors3D.CalcMolDescriptors3D(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_rdkit_all(SMILES:str):\n",
    "    \"Extract chemical features and 3d features from SMILES\"\n",
    "    feat = get_rdkit(SMILES)\n",
    "    feat_3d = get_rdkit_3d(SMILES)\n",
    "    return feat|feat_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_hi_corr(df: pd.DataFrame, \n",
    "                   thr=0.99 # threshold\n",
    "                   ):\n",
    "    \"Remove highly correlated features in a dataframe given a pearson threshold\"\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > thr)]\n",
    "    return df.drop(to_drop, axis=1), to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def preprocess(df: pd.DataFrame, thr=0.99):\n",
    "    \"Remove features with no variance, and highly correlated features based on threshold.\"\n",
    "    col_ori = df.columns\n",
    "\n",
    "    # Remove columns with std == 0\n",
    "    std_zero_cols = df.columns[df.std() == 0].tolist()\n",
    "    \n",
    "    if std_zero_cols:\n",
    "        n=len(std_zero_cols)\n",
    "        print(f\"\\n {n} Columns with zero std: {std_zero_cols}\")\n",
    "    df = df.loc[:, df.std() != 0].copy()\n",
    "\n",
    "    # Remove highly correlated columns\n",
    "    df, high_corr_cols = remove_hi_corr(df, thr)\n",
    "    if high_corr_cols:\n",
    "        n=len(high_corr_cols)\n",
    "        print(f\"\\n {n} Columns removed due to high similarity (pearson>{thr}): {high_corr_cols}\")\n",
    "\n",
    "    dropping_col = set(col_ori) - set(df.columns)\n",
    "    n = len(dropping_col)\n",
    "    print(f\"\\n Total removed columns: {n}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_rdkit_df(df: pd.DataFrame,\n",
    "                 include_3d=False,\n",
    "                 col='SMILES',\n",
    "                 postprocess=False,\n",
    "                 chunksize=128, # for parallel process_map \n",
    "                 ):\n",
    "    \"Extract rdkit features (optionally in parallel with 3D) from SMILES in a df\"\n",
    "    func = get_rdkit_all if include_3d else get_rdkit\n",
    "    smiles_list = df[col].tolist()\n",
    "\n",
    "    features = process_map(func, smiles_list,chunksize=chunksize)\n",
    "\n",
    "    out = pd.DataFrame(features)\n",
    "\n",
    "    if postprocess:\n",
    "        out = StandardScaler().fit_transform(out)\n",
    "        out = preprocess(out) # remove redundant\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=Data.collins.get_antibiotics_2k().head(300)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# feat_raw=get_rdkit_df(df)\n",
    "# feat_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat = get_rdkit_df(df,postprocess=True)\n",
    "# feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morgan fingerprints (ECFP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- [kaggle notebook, scikit-fingerprints](https://www.kaggle.com/code/michaszafarczyk/molecular-fingerprints-using-scikit-fingerprints)\n",
    "- [scikit-fingerprints github](https://github.com/scikit-fingerprints/scikit-fingerprints)\n",
    "- [kaggle notebook, fingerprint tips and tricks](https://www.kaggle.com/code/towardsentropy/fingerprint-tips-and-tricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_fp(SMILES, \n",
    "           name='ecfp',\n",
    "           ELEMENTS_PER_WORKER = 1_000_000):\n",
    "    \"Super fast method to get molecule fingerprints using scikit-fingerprints\"\n",
    "    if name == 'ecfp':\n",
    "        fp_transformer = ECFPFingerprint(fp_size=2048, radius=3, n_jobs=-1)\n",
    "    elif name == 'ap':\n",
    "        fp_transformer = AtomPairFingerprint(fp_size=2048, n_jobs=-1)\n",
    "    elif name == 'maccs':\n",
    "        fp_transformer = MACCSFingerprint(n_jobs=-1)\n",
    "    else:\n",
    "        raise Exception('Wrong fingerprint name!')\n",
    "    \n",
    "    middle_parts = []\n",
    "    k_splits = len(SMILES) // ELEMENTS_PER_WORKER\n",
    "        \n",
    "    for i in tqdm(range(k_splits)):\n",
    "        middle_parts.append(fp_transformer.transform(SMILES[i * ELEMENTS_PER_WORKER: (i + 1) * ELEMENTS_PER_WORKER]))\n",
    "        \n",
    "    if len(SMILES) % ELEMENTS_PER_WORKER > 0:   \n",
    "        middle_parts.append(fp_transformer.transform(SMILES[k_splits * ELEMENTS_PER_WORKER:]))\n",
    "    \n",
    "    return np.concatenate(middle_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# fp = get_fp(df.SMILES, name='ecfp')\n",
    "# fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use np.packbits to save space\n",
    "> Save space when dealing with super big dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packbits will transform binary to decimals, if it is [1, 0, 1, 1, 0, 0, 0, 0, 1, 1], then it will take 8 bits in the sequence, [10110000] and [11000000], for the latter, if not enough for 8 bits length, it will add 0. \n",
    "\n",
    "To transform binary to decimals, take [10110000] for example, calculate through 1*2^7 + 0*2^6 + 1*2^5 + 1*2^4 + 0*2^3 + 0*2^2 + 0*2^1 + 0*2^0=176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compress_fp(array):\n",
    "    \"Compress binary finterprints using np.packbits\"\n",
    "    return np.packbits(array,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress_fp(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanimoto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.kaggle.com/code/towardsentropy/fingerprint-tips-and-tricks/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slow version:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tanimotos_scipy = 1-distance.cdist(fp, fp, metric='jaccard')\n",
    "# # 30.4s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little faster with parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tanimotos_sklearn = 1 - pairwise_distances(fp, metric='jaccard', n_jobs=-1)\n",
    "# # 13.1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import DataStructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tanimotos_rdkit = np.array([DataStructs.BulkTanimotoSimilarity(i, fp) for i in fp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanimoto_bitwise(fps_np):\n",
    "    intersection = (fps_np[:,None] & fps_np[None]).sum(-1)\n",
    "    union = (fps_np[:,None] | fps_np[None]).sum(-1)\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanimoto_bitwise(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba tanimoto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@numba.njit(parallel=True)\n",
    "def tanimoto_numba(fps):\n",
    "    \"Get a NxN matrix of tanimoto similarity among N compounds.\"\n",
    "    n = fps.shape[0]\n",
    "    result = np.zeros((n, n), dtype=np.float32)\n",
    "    \n",
    "    for i in numba.prange(n):\n",
    "        for j in range(i, n):  # only upper triangle\n",
    "            inter = np.bitwise_and(fps[i], fps[j]).sum()\n",
    "            union = np.bitwise_or(fps[i], fps[j]).sum()\n",
    "            sim = inter / union if union > 0 else 0.0\n",
    "            result[i, j] = sim\n",
    "            result[j, i] = sim  # fill symmetric position\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tanimoto_numba(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group same compounds\n",
    "> Utilize hash sha256 to encode morgan fp and find same molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def hash_fp(fp_row):\n",
    "    \"Hash a binary fingerprint row using SHA256\"\n",
    "    return hashlib.sha256(fp_row.tobytes()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash_fp(fp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_same_mol_group(df, smi_col='SMILES'):\n",
    "    \"Assign a group number to the same compounds by utilizing hash sha256 to encode morgan fp and find same molecule.\"\n",
    "    df = df.copy()\n",
    "    smiles = df[smi_col].tolist()\n",
    "    \n",
    "    fps = get_fp(smiles)\n",
    "    \n",
    "    # Hash each fingerprint\n",
    "    # hashes = [hash_fp(fp) for fp in fps] # non-parallel\n",
    "    hashes = process_map(hash_fp, fps, chunksize=256) # if parallel\n",
    "    \n",
    "    # Assign a group number based on hash buckets\n",
    "    hash_to_group = {}\n",
    "    group_ids = []\n",
    "    group_counter = 0\n",
    "    for h in hashes:\n",
    "        if h not in hash_to_group:\n",
    "            hash_to_group[h] = group_counter\n",
    "            group_counter += 1\n",
    "        group_ids.append(hash_to_group[h])\n",
    "    \n",
    "    df['group'] = group_ids\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df2 = get_same_mol_group(df)\n",
    "# df2.group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reduce_feature(data, # df or numpy array\n",
    "                   method='pca', # dimensionality reduction method, accept both capital and lower case\n",
    "                   complexity=20, # None for PCA; perfplexity for TSNE, recommend: 30; n_neigbors for UMAP, recommend: 15\n",
    "                   n=2, # n_components\n",
    "                   seed: int=123, # seed for random_state\n",
    "                   **kwargs, # arguments from PCA, TSNE, or UMAP depends on which method to use\n",
    "                  ):\n",
    "    \n",
    "    \"Reduce the dimensionality given a dataframe of values\"\n",
    "    \n",
    "    method = method.lower()\n",
    "    assert method in ['pca','tsne','umap'], \"Please choose a method among PCA, TSNE, and UMAP\"\n",
    "\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n, random_state=seed,**kwargs)\n",
    "    elif method == 'tsne':\n",
    "        reducer = TSNE(n_components=n,\n",
    "                       random_state=seed, \n",
    "                       perplexity = complexity, # default from official is 30 \n",
    "                      **kwargs)\n",
    "    elif method == 'umap':\n",
    "        reducer = UMAP(n_components=n, \n",
    "                       random_state=seed, \n",
    "                       n_neighbors=complexity, # default from official is 15, try 15-200\n",
    "                      **kwargs)\n",
    "    else:\n",
    "        raise ValueError('Invalid method specified')\n",
    "\n",
    "    proj = reducer.fit_transform(data)\n",
    "    embedding_df = pd.DataFrame(proj).set_index(data.index) if isinstance(data,pd.DataFrame) else pd.DataFrame(proj)\n",
    "    embedding_df.columns = [f\"{method.upper()}{i}\" for i in range(1, n + 1)]\n",
    "\n",
    "    return embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = reduce_feature(fp,'pca',n=10)\n",
    "# pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# #| export\n",
    "# def tanimoto(df, # df with SMILES and ID columns\n",
    "#              smiles_col='SMILES', # colname of SMILES\n",
    "#              id_col='ID', # colname of compound ID\n",
    "#              target_col=None, # colname of compound values (e.g., IC50)\n",
    "#              radius=2, # radius of the Morgan fingerprint.\n",
    "#              ):\n",
    "#     \"Calculates the Tanimoto similarity scores between all pairs of molecules in a pandas DataFrame.\"\n",
    "    \n",
    "#     df = df.copy()\n",
    "#     # Convert SMILES to molecule objects\n",
    "#     df['Molecule'] = df[smiles_col].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "\n",
    "#     # Calculate fingerprints\n",
    "#     df['Fingerprint'] = df['Molecule'].apply(lambda x: AllChem.GetMorganFingerprintAsBitVect(x, radius))\n",
    "\n",
    "#     # Calculate similarity scores\n",
    "#     similarity_scores = []\n",
    "#     for i in range(len(df)):\n",
    "#         for j in range(i+1, len(df)):\n",
    "#             sim_score = DataStructs.TanimotoSimilarity(df['Fingerprint'][i], df['Fingerprint'][j])\n",
    "#             if target_col is not None:\n",
    "#                 similarity_scores.append((df[id_col][i], df[id_col][j], df[smiles_col][i], df[smiles_col][j], sim_score, df[target_col][i], df[target_col][j]))\n",
    "#             else:\n",
    "#                 similarity_scores.append((df[id_col][i], df[id_col][j], df[smiles_col][i], df[smiles_col][j], sim_score))\n",
    "\n",
    "#     # Create a new DataFrame with the similarity scores\n",
    "#     if target_col is not None:\n",
    "#         result_df = pd.DataFrame(similarity_scores, columns=['ID1', 'ID2', 'SMILES1', 'SMILES2', 'SimilarityScore', 'Target1', 'Target2'])\n",
    "#     else:\n",
    "#         result_df = pd.DataFrame(similarity_scores, columns=['ID1', 'ID2', 'SMILES1', 'SMILES2', 'SimilarityScore'])\n",
    "\n",
    "#     # Sort by similarity score in descending order\n",
    "#     result_df.sort_values('SimilarityScore', ascending=False, inplace=True)\n",
    "#     result_df = result_df.reset_index(drop=True)\n",
    "\n",
    "#     return result_df\n",
    "\n",
    "# df = Data.get_mirati_g12d_raw()[['ID','SMILES','IC50']]\n",
    "# df = df.dropna(subset= 'IC50').reset_index(drop=True)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# # result = tanimoto(df.head(), target_col = 'IC50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
