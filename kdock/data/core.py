# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/data/00_core.ipynb.

# %% auto 0
__all__ = ['Data', 'get_uniprot_seq', 'get_uniprot_features', 'get_uniprot_kd', 'get_uniprot_type', 'mutate', 'compare_seq',
           'rglob', 'copy_files', 'rdkit_conformer', 'get_rec_lig', 'get_box', 'tanimoto']

# %% ../../nbs/data/00_core.ipynb 4
# basics
import requests, subprocess,shutil,zipfile
import pandas as pd, numpy as np
from functools import lru_cache
from pathlib import Path
from fastcore.meta import delegates
# from tqdm import tqdm
# tqdm.pandas()
from tqdm.contrib.concurrent import process_map
pd.set_option('mode.chained_assignment', 'raise') # raise error when overwriting

# rdkit
from rdkit import Chem,RDLogger
from rdkit.ML.Descriptors import MoleculeDescriptors
from rdkit.Chem import Draw,Descriptors,Descriptors3D, AllChem,rdFingerprintGenerator
RDLogger.DisableLog('rdApp.warning')

from sklearn.preprocessing import StandardScaler
from sklearn import set_config
set_config(transform_output="pandas")

# %% ../../nbs/data/00_core.ipynb 6
class Data:
    "A class for fetching various datasets."

    @staticmethod
    @lru_cache()
    def fetch_csv(url): return pd.read_csv(url)

    class collins:
        
        @staticmethod
        def get_antibiotics_2k():
            """
            Antibiotics dataset of 50 µM 2,560 compounds screening in E. coli K12 BW25113.
            2,335 unique compounds after deduplicated.
            Table S1B from 2020 Cell: A Deep Learning Approach to Antibiotic Discovery.
            """
            URL = "https://github.com/sky1ove/kdock/raw/main/dataset/antibiotics_2k.csv"
            return Data.fetch_csv(URL)

        @staticmethod
        def get_antibiotics_39k():
            """
            Antibiotics dataset of 50 µM 39,128 compounds screening in E. coli K12 BW25113.
            Supplementary dataset EV1 from 2022 Molecular Systems Biology: Benchmarking AlphaFold-enabled molecular docking predictions for antibiotic discovery.
            """
            URL = "https://github.com/sky1ove/kdock/raw/main/dataset/antibiotics_39k.csv"
            return Data.fetch_csv(URL)

        @staticmethod
        def get_antibiotics_enzyme():
            """
            Antibiotics enzymatic inhibition dataset of 100 µM 218 compounds and 12 essential proteins in E. coli K12 BW25113.
            Flattened benchmark dataset/Supplementary EV4 from 2022 Molecular Systems Biology: Benchmarking AlphaFold-enabled molecular docking predictions for antibiotic discovery.
            """
            URL = "https://github.com/sky1ove/kdock/raw/main/dataset/antibiotics_enzyme.csv"
            return Data.fetch_csv(URL)



    class kras:
    
        @staticmethod
        def get_mirati_g12d():
            "Deduplicated G12D dataset from the mirati paper and patents."
            URL = "https://github.com/sky1ove/kdock/raw/main/dataset/KRASi_g12d_dedup.csv"
            return Data.fetch_csv(URL)
        
        @staticmethod
        def get_mirati_g12d_raw():
            "Raw G12D dataset from the paper and patents without deduplication"
            URL = "https://github.com/sky1ove/kdock/raw/main/dataset/KRASi_g12d.csv"
            return Data.fetch_csv(URL)
        
        @staticmethod
        def get_seq():
            "Protein sequence of human KRAS and its mutants G12D and G12C."
            URL = "https://github.com/sky1ove/kdock/raw/main/dataset/kras_seq.csv"
            return Data.fetch_csv(URL)

# %% ../../nbs/data/00_core.ipynb 17
@lru_cache()
def get_uniprot_seq(uniprot_id):
    "Queries the UniProt database to retrieve the protein sequence for a given UniProt ID."
    
    url = f"https://www.uniprot.org/uniprot/{uniprot_id}.fasta"
    response = requests.get(url)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        data = response.text
        # The sequence starts after the first line, which is a description
        sequence = ''.join(data.split('\n')[1:]).strip()
        return sequence
    else:
        return f"Error: Unable to retrieve sequence for UniProt ID {uniprot_id}. Status code: {response.status_code}"

# %% ../../nbs/data/00_core.ipynb 19
@lru_cache()
def get_uniprot_features(uniprot_id):
    "Given uniprot_id, get specific region for uniprot features."
    # uniprot REST API
    url = f"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json"
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
        # Extract the "features" section which contains information
        features = data.get('features', [])
        return features
    else:
        raise ValueError(f"Failed to retrieve UniProt features for {uniprot_id}")

# %% ../../nbs/data/00_core.ipynb 21
def get_uniprot_kd(uniprot_id):
    "Get kinase domain sequences based on UniProt ID."
    features = get_uniprot_features(uniprot_id)
    out_regions = []
    seq = get_uniprot_seq(uniprot_id)

    for feature in features:
        if feature.get("type") == "Domain" and "Protein kinase" in feature.get("description", ""):
            start = feature['location']['start']['value']
            end = feature['location']['end']['value']
            region = {
                'uniprot_id': uniprot_id,
                'type': feature['type'],
                'start': start,
                'end': end,
                'description': feature['description'],
                'sequence': seq[start-1:end]
            }
            out_regions.append(region)

    return out_regions

# %% ../../nbs/data/00_core.ipynb 23
def get_uniprot_type(uniprot_id,type_='Signal'):
    "Get region sequences based on UniProt ID features."
    features = get_uniprot_features(uniprot_id)
    out_regions = []
    seq = get_uniprot_seq(uniprot_id)

    for feature in features:
        if feature.get("type") == type_:
            start = feature['location']['start']['value']
            end = feature['location']['end']['value']
            region = {
                'uniprot_id': uniprot_id,
                'type': feature['type'],
                'start': start,
                'end': end,
                'description': feature['description'],
                'sequence': seq[start-1:end]
            }
            out_regions.append(region)

    return out_regions

# %% ../../nbs/data/00_core.ipynb 27
def mutate(seq, # protein sequence
           *mutations, # e.g., E709A
           verbose=True,
           ):
    "Apply mutations to a protein sequence."
    seq_list = list(seq)  # convert to list for mutability
    
    for mut in mutations:
        # check mutation format
        if len(mut) < 3: raise ValueError(f"Invalid mutation format: {mut}")
        
        from_aa,pos,to_aa = mut[0],int(mut[1:-1])-1,mut[-1]

        # make sure position is within the sequence length
        if pos < 0 or pos >= len(seq_list): raise IndexError(f"Position {pos + 1} out of range for sequence length {len(seq_list)}")
        # make sure aa from mutations matches the residue on the sequence
        if seq_list[pos] != from_aa: raise ValueError(f"Expected {from_aa} at position {pos + 1}, found {seq_list[pos]}")
        
        seq_list[pos] = to_aa
        if verbose: print('Converted:', mut)
        
    return ''.join(seq_list)

# %% ../../nbs/data/00_core.ipynb 29
def compare_seq(original_seq, mutated_seq):
    "Compare original and mutated sequences."
    
    if len(original_seq) != len(mutated_seq): raise ValueError("Sequences must be the same length to compare.")

    differences = []
    for i, (orig, mut) in enumerate(zip(original_seq, mutated_seq), start=1):
        if orig != mut:
            differences.append((i, orig, mut))

    if not differences: print("No differences found. Sequences are identical.")
    else:
        print("Differences found at positions:")
        for pos, orig, mut in differences:
            print(f"  Position {pos}: {orig} → {mut}")

# %% ../../nbs/data/00_core.ipynb 33
def rglob(path, pattern, max_depth):
    "Get a file list given folder depths"
    base_path = Path(path).resolve()
    for path in base_path.rglob(pattern):
        if len(path.relative_to(base_path).parts) <= max_depth:
            yield path

# %% ../../nbs/data/00_core.ipynb 35
def copy_files(file_list, dest_dir):
    "Copy a list of files to the destination directory, or zip them if dest_dir ends with .zip."
    dest_path = Path(dest_dir)

    if dest_path.suffix == ".zip":
        with zipfile.ZipFile(dest_path, 'w') as zipf:
            for file_path in file_list:
                file_path = Path(file_path)
                zipf.write(file_path, arcname=file_path.name)
        print(f'Zipped {len(file_list)} files to {dest_path}')
    else:
        dest_path.mkdir(parents=True, exist_ok=True)
        for file_path in file_list:
            file_path = Path(file_path)
            shutil.copy2(file_path, dest_path / file_path.name)
        print(f'Copied {len(file_list)} files to {dest_path}')

# %% ../../nbs/data/00_core.ipynb 38
def rdkit_conformer(SMILES, # SMILES string
                    output=None, # file ".sdf" to be saved
                    method='ETKDG', # Optimization method, can be 'UFF', 'MMFF' or 'ETKDGv3'
                    visualize=True, #whether or not to visualize the compound
                    seed = 3,# randomness of the 3D conformation
                    ):

    "Gemerate 3D conformers from SMILES"
    
    np.random.seed(seed) 
    mol = Chem.MolFromSmiles(SMILES)
    
    # Generate a 3D conformation of the molecule
    AllChem.EmbedMolecule(mol)
    

    # Optimize the 3D conformation using the specified force field method
    if method == 'UFF':
        AllChem.UFFOptimizeMolecule(mol)
    elif method == 'MMFF':
        AllChem.MMFFOptimizeMolecule(mol)
    elif method == 'ETKDG':
        AllChem.EmbedMultipleConfs(mol, numConfs=1, useExpTorsionAnglePrefs=True, 
                                   useBasicKnowledge=True, enforceChirality=True,randomSeed=seed)
        AllChem.ETKDGv3()
        AllChem.UFFOptimizeMolecule(mol)

    else:
        raise ValueError('Invalid method specified')
        

    # Remove hydrogens from the molecule
    # mol = Chem.RemoveHs(mol)

    if output is not None:
        Path(output).parent.mkdir(parents=True,exist_ok=True)
    
        w = Chem.SDWriter(output)
        w.write(mol)
        w.close()
    return mol

# %% ../../nbs/data/00_core.ipynb 41
def get_rec_lig(pdb_id: str, # pdb id for download
                            lig_id: str, # ligand id shown on the protein page
                            out_dir = '.', # directory path to save pdb files
                            ):
    "Download pdb and extract receptor and ligand from a PDB ID."
    out_dir = Path(out_dir).expanduser().resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    pdb_file = out_dir / f"{pdb_id}.pdb"
    rec_file = out_dir / f"{pdb_id}_receptor.pdb"
    lig_pdb_file = out_dir / f"{pdb_id}_lig.pdb"
    lig_sdf_file = out_dir / f"{pdb_id}_lig.sdf"

    # Download if not exists
    if not pdb_file.exists():
        url = f"http://files.rcsb.org/download/{pdb_id}.pdb"
        print(f'Downloading pdb: {pdb_id}')
        subprocess.run(["wget", url, "-O", str(pdb_file)], check=True,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    print(f'{pdb_id}.pdb is detected!')

    # Extract protein (all ATOM lines excluding ligand ID)
    with open(pdb_file) as infile, open(rec_file, 'w') as out_rec:
        for line in infile:
            if line.startswith("ATOM") and lig_id not in line:
                out_rec.write(line)

    # Extract ligand
    with open(pdb_file) as infile, open(lig_pdb_file, 'w') as out_lig:
        for line in infile:
            if lig_id in line and line.startswith(("HETATM", "ATOM")):
                out_lig.write(line)

    # Convert ligand PDB to SDF using RDKit
    mol = Chem.MolFromPDBFile(str(lig_pdb_file), removeHs=False)
    if mol is None:
        raise ValueError("Failed to parse ligand from PDB.")
    
    writer = Chem.SDWriter(str(lig_sdf_file))
    writer.write(mol)
    writer.close()

    return str(rec_file), str(lig_sdf_file)

# %% ../../nbs/data/00_core.ipynb 44
def get_box(sdf_file, autobox_add=4.0,tolist=False):
    "Get the box coordinates of ligand.sdf; mimic GNINA's --autobox_ligand behavior."
    mol = Chem.SDMolSupplier(str(sdf_file), removeHs=False)[0]
    if mol is None:
        raise ValueError(f"Failed to read molecule from {sdf_file}")
    
    conf = mol.GetConformer()
    coords = np.array([list(conf.GetAtomPosition(i)) for i in range(mol.GetNumAtoms())])
    
    min_coords = coords.min(axis=0)
    max_coords = coords.max(axis=0)
    
    center = (min_coords + max_coords) / 2
    size = (max_coords - min_coords) + autobox_add

    box_dict = {
        "center_x": round(float(center[0]), 3),
        "center_y": round(float(center[1]), 3),
        "center_z": round(float(center[2]), 3),
        "size_x": round(float(size[0]), 3),
        "size_y": round(float(size[1]), 3),
        "size_z": round(float(size[2]), 3)
    }
    return list(box_dict.values()) if tolist else box_dict

# %% ../../nbs/data/00_core.ipynb 47
def tanimoto(df, # df with SMILES and ID columns
             smiles_col='SMILES', # colname of SMILES
             id_col='ID', # colname of compound ID
             target_col=None, # colname of compound values (e.g., IC50)
             radius=2, # radius of the Morgan fingerprint.
             ):
    "Calculates the Tanimoto similarity scores between all pairs of molecules in a pandas DataFrame."
    
    df = df.copy()
    # Convert SMILES to molecule objects
    df['Molecule'] = df[smiles_col].apply(lambda x: Chem.MolFromSmiles(x))

    # Calculate fingerprints
    df['Fingerprint'] = df['Molecule'].apply(lambda x: AllChem.GetMorganFingerprintAsBitVect(x, radius))

    # Calculate similarity scores
    similarity_scores = []
    for i in range(len(df)):
        for j in range(i+1, len(df)):
            sim_score = DataStructs.TanimotoSimilarity(df['Fingerprint'][i], df['Fingerprint'][j])
            if target_col is not None:
                similarity_scores.append((df[id_col][i], df[id_col][j], df[smiles_col][i], df[smiles_col][j], sim_score, df[target_col][i], df[target_col][j]))
            else:
                similarity_scores.append((df[id_col][i], df[id_col][j], df[smiles_col][i], df[smiles_col][j], sim_score))

    # Create a new DataFrame with the similarity scores
    if target_col is not None:
        result_df = pd.DataFrame(similarity_scores, columns=['ID1', 'ID2', 'SMILES1', 'SMILES2', 'SimilarityScore', 'Target1', 'Target2'])
    else:
        result_df = pd.DataFrame(similarity_scores, columns=['ID1', 'ID2', 'SMILES1', 'SMILES2', 'SimilarityScore'])

    # Sort by similarity score in descending order
    result_df.sort_values('SimilarityScore', ascending=False, inplace=True)
    result_df = result_df.reset_index(drop=True)

    return result_df
